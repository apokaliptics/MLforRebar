---
title: "On the Dangers of Bootstrapping Generation for Continual Learning and Beyond"
authors: "Daniil Zverev, A. Sophia Koepke, Joao F. Henriques"
categories: "cs.LG, cs.AI, cs.CV, eess.IV"
source: "http://arxiv.org/abs/2512.11867v1"
pdf: "https://arxiv.org/pdf/2512.11867v1"
---

## Abstract

The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.
