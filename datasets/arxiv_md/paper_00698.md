---
title: "How Smoothing is N-simplicial Attention?"
authors: "Alexandre Dussolle, Pietro Li√≤"
categories: "cs.LG, cs.AI"
source: "http://arxiv.org/abs/2512.15600v1"
pdf: "https://arxiv.org/pdf/2512.15600v1"
---

## Abstract

Going from pure Multilayer Perceptron (MLP) to a learnable graph message-passing mechanism at each layer has been foundational to state-of-the-art results, despite the computational trade-off (e.g. GATs or Transformers). To go a step further, in this work, we introduce N-simplicial attention, going from pairwise token similarity to higher-order interactions, and adapt it for Rotary Position Embeddings (RoPE). To help manage the increased complexity, we propose a cost-effective simplex selection enabling the model to focus its computation load onto the more task-sensitive interactions. Beyond these core mechanisms, we study how smoothing N-simplicial attention is by deriving a Lipschitz upper-bound and by demonstrating that by itself it also suffers from over-smoothing, despite opening the attention message-passing to higher-order interactions.
