---
title: "VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling"
authors: "Weiqi Li, Quande Zhang, Ruifeng Zhai, Liang Lin, Guangrun Wang"
categories: "cs.RO, cs.AI, cs.LG"
source: "http://arxiv.org/abs/2512.02902v1"
pdf: "https://arxiv.org/pdf/2512.02902v1"
---

## Abstract

Vision-language-action (VLA) models achieve strong in-distribution performance but degrade sharply under novel camera viewpoints and visual perturbations. We show that this brittleness primarily arises from misalignment in Spatial Modeling, rather than Physical Modeling. To address this, we propose a one-shot adaptation framework that recalibrates visual representations through lightweight, learnable updates. Our first method, Feature Token Modulation (FTM), applies a global affine transformation to visual tokens and improves Libero viewpoint accuracy from 48.5% to 87.1% with only 4K parameters. Building on this, Feature Linear Adaptation (FLA) introduces low-rank updates to the ViT encoder, achieving 90.8% success with 4.7M parameters -- matching LoRA-scale finetuning at far lower cost. Together, these results reveal substantial untapped robustness in pretrained VLA models and demonstrate that targeted, minimal visual adaptation is sufficient to restore viewpoint generalization.
