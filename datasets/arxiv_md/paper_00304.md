---
title: "PTTA: A Pure Text-to-Animation Framework for High-Quality Creation"
authors: "Ruiqi Chen, Kaitong Cai, Yijia Fan, Keze Wang"
categories: "cs.CV, cs.AI"
source: "http://arxiv.org/abs/2512.18614v1"
pdf: "https://arxiv.org/pdf/2512.18614v1"
---

## Abstract

Traditional animation production involves complex pipelines and significant manual labor cost. While recent video generation models such as Sora, Kling, and CogVideoX achieve impressive results on natural video synthesis, they exhibit notable limitations when applied to animation generation. Recent efforts, such as AniSora, demonstrate promising performance by fine-tuning image-to-video models for animation styles, yet analogous exploration in the text-to-video setting remains limited.
  In this work, we present PTTA, a pure text-to-animation framework for high-quality animation creation. We first construct a small-scale but high-quality paired dataset of animation videos and textual descriptions. Building upon the pretrained text-to-video model HunyuanVideo, we perform fine-tuning to adapt it to animation-style generation. Extensive visual evaluations across multiple dimensions show that the proposed approach consistently outperforms comparable baselines in animation video synthesis.
