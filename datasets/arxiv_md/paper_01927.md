---
title: "Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots"
authors: "Jihyung Park, Saleh Afroogh, Junfeng Jiao"
categories: "cs.CL, cs.AI"
source: "http://arxiv.org/abs/2512.06193v1"
pdf: "https://arxiv.org/pdf/2512.06193v1"
---

## Abstract

Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.
