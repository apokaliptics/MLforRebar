---
title: "Auto-BenchmarkCard: Automated Synthesis of Benchmark Documentation"
authors: "Aris Hofmann, Inge Vejsbjerg, Dhaval Salwala, Elizabeth M. Daly"
categories: "cs.HC, cs.AI"
source: "http://arxiv.org/abs/2512.09577v1"
pdf: "https://arxiv.org/pdf/2512.09577v1"
---

## Abstract

We present Auto-BenchmarkCard, a workflow for generating validated descriptions of AI benchmarks. Benchmark documentation is often incomplete or inconsistent, making it difficult to interpret and compare benchmarks across tasks or domains. Auto-BenchmarkCard addresses this gap by combining multi-agent data extraction from heterogeneous sources (e.g., Hugging Face, Unitxt, academic papers) with LLM-driven synthesis. A validation phase evaluates factual accuracy through atomic entailment scoring using the FactReasoner tool. This workflow has the potential to promote transparency, comparability, and reusability in AI benchmark reporting, enabling researchers and practitioners to better navigate and evaluate benchmark choices.
