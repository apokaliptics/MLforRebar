---
title: "Sparse Attention Post-Training for Mechanistic Interpretability"
authors: "Florent Draye, Anson Lei, Ingmar Posner, Bernhard Sch√∂lkopf"
categories: "cs.LG, cs.AI"
source: "http://arxiv.org/abs/2512.05865v1"
pdf: "https://arxiv.org/pdf/2512.05865v1"
---

## Abstract

We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\approx 0.3 \%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.
