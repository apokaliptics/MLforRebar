---
title: "Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control"
authors: "Nathan P. Lawrence, Ali Mesbah"
categories: "cs.LG, cs.AI"
source: "http://arxiv.org/abs/2512.06471v1"
pdf: "https://arxiv.org/pdf/2512.06471v1"
---

## Abstract

Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.
