---
title: "David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?"
authors: "Shashwat Shankar, Subhranshu Pandey, Innocent Dengkhw Mochahari, Bhabesh Mali, Animesh Basak Chowdhury, Sukanta Bhattacharjee, Chandan Karfa"
categories: "cs.LG, cs.AI, cs.AR, cs.SE"
source: "http://arxiv.org/abs/2512.05073v1"
pdf: "https://arxiv.org/pdf/2512.05073v1"
---

## Abstract

Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.
