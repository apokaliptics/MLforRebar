---
title: "LPCD: Unified Framework from Layer-Wise to Submodule Quantization"
authors: "Yuma Ichikawa, Yudai Fujimoto, Akira Sakai"
categories: "stat.ML, cs.AI, cs.CL, cs.LG"
source: "http://arxiv.org/abs/2512.01546v1"
pdf: "https://arxiv.org/pdf/2512.01546v1"
---

## Abstract

Post-training quantization (PTQ) aims to preserve model-level behavior; however, most methods focus on individual linear layers. Even recent extensions, such as QEP and LoaQ, which mitigate error propagation or target specific submodules, still rely on layer-wise formulations and fail to capture the behavior of larger submodules. We introduce Layer-Projected Coordinate Descent (LPCD), a unified framework that extends PTQ beyond layers by optimizing relaxed objectives across arbitrary submodules and projecting the solutions with layer-wise quantizers. LPCD generalizes existing methods and provides a principled approach to quantizing complex submodules while maintaining the efficiency and compatibility of layer-wise PTQ pipelines. Across diverse LLM architectures and bit-widths, LPCD-based submodule quantization consistently enhances both layer-wise PTQ methods and existing submodule approaches.
