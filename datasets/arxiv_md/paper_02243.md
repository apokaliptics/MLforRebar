---
title: "Jina-VLM: Small Multilingual Vision Language Model"
authors: "Andreas Koukounas, Georgios Mastrapas, Florian HÃ¶nicke, Sedigheh Eslami, Guillaume Roncari, Scott Martens, Han Xiao"
categories: "cs.CL, cs.AI, cs.CV"
source: "http://arxiv.org/abs/2512.04032v2"
pdf: "https://arxiv.org/pdf/2512.04032v2"
---

## Abstract

We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. The model achieves leading results on standard VQA benchmarks and multilingual evaluations while preserving competitive text-only performance. Model weights and code are publicly released at https://huggingface.co/jinaai/jina-vlm .
