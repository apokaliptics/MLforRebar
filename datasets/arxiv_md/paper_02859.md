---
title: "Resolving Conflicts in Lifelong Learning via Aligning Updates in Subspaces"
authors: "Yueer Zhou, Yichen Wu, Ying Wei"
categories: "cs.LG, cs.AI, cs.CL"
source: "http://arxiv.org/abs/2512.08960v1"
pdf: "https://arxiv.org/pdf/2512.08960v1"
---

## Abstract

Low-Rank Adaptation (LoRA) enables efficient Continual Learning but often suffers from catastrophic forgetting due to destructive interference between tasks. Our analysis reveals that this degradation is primarily driven by antagonistic directional updates where new task gradients directly oppose the historical weight trajectory. To address this, we propose PS-LoRA (Parameter Stability LoRA), a framework designed to resolve conflicts by aligning updates within the optimization subspace. Our approach employs a dual-regularization objective that penalizes conflicting directions and constrains magnitude deviations to ensure consistency with prior knowledge. Additionally, we implement a magnitude-based merging strategy to consolidate sequential adapters into a robust representation without retraining. Experiments on NLP and Vision benchmarks show that PS-LoRA outperforms state-of-the-art methods by preserving the stability of learned representations while efficiently adapting to new domains.
