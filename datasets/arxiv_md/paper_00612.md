---
title: "CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity"
authors: "Jinhao Zhang, Yunquan Zhang, Daning Chen"
categories: "cs.LG, cs.AI"
source: "http://arxiv.org/abs/2512.16282v1"
pdf: "https://arxiv.org/pdf/2512.16282v1"
---

## Abstract

Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quantization, a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. Our method independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMs including LLaMA and Qwen ,in terms of perplexity (PPL) and downstream task performance.
