---
title: "CLIP-RL: Aligning Language and Policy Representations for Task Transfer in Reinforcement Learning"
authors: "Chainesh Gautam, Raghuram Bharadwaj Diddigi"
categories: "cs.AI"
source: "http://arxiv.org/abs/2512.01616v1"
pdf: "https://arxiv.org/pdf/2512.01616v1"
---

## Abstract

Recently, there has been an increasing need to develop agents capable of solving multiple tasks within the same environment, especially when these tasks are naturally associated with language. In this work, we propose a novel approach that leverages combinations of pre-trained (language, policy) pairs to establish an efficient transfer pipeline. Our algorithm is inspired by the principles of Contrastive Language-Image Pretraining (CLIP) in Computer Vision, which aligns representations across different modalities under the philosophy that ''two modalities representing the same concept should have similar representations.'' The central idea here is that the instruction and corresponding policy of a task represent the same concept, the task itself, in two different modalities. Therefore, by extending the idea of CLIP to RL, our method creates a unified representation space for natural language and policy embeddings. Experimental results demonstrate the utility of our algorithm in achieving faster transfer across tasks.
